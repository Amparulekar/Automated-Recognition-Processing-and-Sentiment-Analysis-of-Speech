{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfDVGhsCYWKx"
      },
      "source": [
        "# LAB 3: Speech-to-text with RNN\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/556/1*NhOH4X9wKWfO6o8faYFf-w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBNioI7dZNWg"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this lab, we are going to use Recurrent Neural Networks to do some Speech Recognition.\n",
        "\n",
        "Nowadays, speech recognition is a common task present in smart home assistants (Amazon Echo, Google Home), phones, TVs... Most of the time, it is done using deep learning.\n",
        "\n",
        "## What you will learn\n",
        "\n",
        "- The different kinds of RNN (RNN, LSTM, GRU...)\n",
        "\n",
        "- How to load and process audio data in PyTorch\n",
        "\n",
        "- How to implement an RNN in PyTorch\n",
        "\n",
        "- How to create a confusion matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9M96xtPSw2g"
      },
      "source": [
        "## RNN\n",
        "\n",
        "Recurrent Neural Networks are a kind of Neural Network used to process **sequences** of data.\n",
        "These sequences can be of varying length and usually have some context information.\n",
        "\n",
        "For example, sentences (text), audio, videos have some temporal logic. In a sentence, one word depends on the word that comes before it. In a video, one frame probably looks a lot like the previous frames.\n",
        "\n",
        "RNNs have some kind of **persistence** of information during the processing of a sequence. Thus, RNNs are used for lots of things: sentiment analysis, text completion, speech recognition, etc.\n",
        "\n",
        "![alt text](https://www.researchgate.net/profile/Weijiang_Feng/publication/318332317/figure/fig1/AS:614309562437664@1523474221928/The-standard-RNN-and-unfolded-RNN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4K3YG1Dxml6"
      },
      "source": [
        "## LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit)\n",
        "\n",
        "**RNN Short-term memory problem:**\n",
        "\n",
        "*   Recurrent Neural Networks suffer from short-term memory. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So if you are trying to process a paragraph of text to do predictions, RNN’s may leave out important information from the beginning.\n",
        "\n",
        "*  During back propagation, recurrent neural networks suffer from the vanishing gradient problem. Gradients are values used to update a neural networks weights. The vanishing gradient problem is when the gradient shrinks as it back propagates through time. If a gradient value becomes extremely small, it doesn’t contribute too much to the learning.\n",
        "\n",
        "**As solution to short-term memory, LSTM and GRU were created:**\n",
        "\n",
        "*   LSTM was introduced by this [article](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
        "*   GRU was introduced by this [article](https://arxiv.org/pdf/1412.3555.pdf).\n",
        "\n",
        "Both are **Reccurent Neural Network (RNN)** architectures which were created as the solution to short-term memory. They have internal mechanisms called gates that can regulate the flow of information.\n",
        "\n",
        "![alt text](http://dprogrammer.org/wp-content/uploads/2019/04/RNN-vs-LSTM-vs-GRU-1200x361.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMak8L4YEtmq"
      },
      "source": [
        "These gates can learn which data in a sequence is important to keep or throw away. By doing that, it can pass relevant information down the long chain of sequences to make predictions.\n",
        "\n",
        "**LSTM**\n",
        "\n",
        "The LSTMs does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\n",
        "\n",
        "*   the **cell state** make easy for the information to pass through the cell by combining the cells decisions\n",
        "*   the **forget gate** decides what information should be thrown away from the cell state\n",
        "*   the **input gate** decides which values we'll update using sigmoid ; it's combined with a tanh layer to create an update to the state\n",
        "*   the **output gate**, based on the celle state, output a filtered information\n",
        "\n",
        "**GRU**\n",
        "\n",
        "The GRU is a modified version of the LSTM. It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOHdDmVzDDIZ"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "Since we are working with PyTorch and sounds, we are going to use *torchaudio* instead of *torchvision*, this time.\n",
        "\n",
        "Make sure you are using a GPU Runtime! (Runtime -> Change Runtime type)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui0IUVuRZCc8"
      },
      "source": [
        "!pip install torchaudio==0.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f31ZpwzCY-dz"
      },
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "## PyTorch things\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## Other libs\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm_notebook\n",
        "import torchsummary\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "import seaborn as sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JnFUmXxYiGt"
      },
      "source": [
        "# Part 1: Working with audio data\n",
        "\n",
        "The dataset we are using is Google's Speech Dataset (https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html).\n",
        "\n",
        "It is composed of **\"65,000 one-second long utterances of 30 short words, by thousands of different people\"**.\n",
        "\n",
        "Let's download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1nRgZsWYbxc"
      },
      "source": [
        "!rm -rf ./*\n",
        "!wget -O speech_commands_v0.01.tar.gz http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
        "!tar xzf speech_commands_v0.01.tar.gz\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1OEu9nuSgWV"
      },
      "source": [
        "Let's print the different classes (words) that are part of this dataset.\n",
        "\n",
        "We can see there are 30 different words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CedTjPtSaT7K"
      },
      "source": [
        "classes = os.listdir()\n",
        "classes.remove(\"LICENSE\")\n",
        "classes.remove(\"README.md\")\n",
        "classes.remove(\"_background_noise_\")\n",
        "classes.remove(\"speech_commands_v0.01.tar.gz\")\n",
        "classes.remove(\"testing_list.txt\")\n",
        "classes.remove(\"validation_list.txt\")\n",
        "classes.remove(\".config\")\n",
        "print(classes)\n",
        "print(\"Number of classes\", len(classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tMAntg5cA5h"
      },
      "source": [
        "## Q1: Listen to some samples\n",
        "\n",
        "Using the **Audio(filename)** function from IPython notebook, you can listen to an audio file directly in Colab.\n",
        "\n",
        "Try it on some samples!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2zMtc99iHJY"
      },
      "source": [
        "Audio(\"bed/1528225c_nohash_2.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ISm2faizpf"
      },
      "source": [
        "Audio(## Try another file here)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25WgIgbncJq9"
      },
      "source": [
        "## Q2: Displaying a waveform\n",
        "\n",
        "Use **torchaudio.load** to load an audio file. Then, use matplotlib to display it.\n",
        "\n",
        "HINT: you may have to transpose the waveform with **.t()** in order to display it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJmE6p_cjXMT"
      },
      "source": [
        "## YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkmYb0GhcQbk"
      },
      "source": [
        "## Computing MFCC features\n",
        "\n",
        "Extracting MFCC (**Mel Frequency Cepstral Coefficients**) features is a well known signal processing technique, especially used in **ASR** (Automatic Speech Recognition). These features are meant to represent the way humans perceive sound. https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n",
        "\n",
        "*Torchaudio* has transforms (just like the ones in *torchvision*) that allow you to compute these features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAgWaItCkBdK"
      },
      "source": [
        "mfcc = torchaudio.transforms.MFCC(n_mfcc=12, log_mels=True)(waveform)\n",
        "plt.figure()\n",
        "plt.imshow(mfcc[0].detach().numpy())\n",
        "print(mfcc.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4DgzPVMza78"
      },
      "source": [
        "Here, we are only keeping 12 MFCC features because it is enough for our purposes.\n",
        "\n",
        "As you can see, we are getting a Tensor of shape [1, 12, 81], because we have one audio channel (mono) with 12 coefficients over 81 time windows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbBmOyI1cVS-"
      },
      "source": [
        "## Creating a custom audio Dataset\n",
        "\n",
        "You may have noticed that in this dataset, the test and validation datasets are given in testing_list.txt and validation_list.txt files.\n",
        "\n",
        "With that, we can infer a training list as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xy92F47oCnz"
      },
      "source": [
        "## Read the test list\n",
        "with open(\"testing_list.txt\") as testing_f:\n",
        "  testing_list = [x.strip() for x in testing_f.readlines()]\n",
        "\n",
        "## Read the val list\n",
        "with open(\"validation_list.txt\") as val_f:\n",
        "  validation_list = [x.strip() for x in val_f.readlines()]\n",
        "\n",
        "print(\"Number of testing samples\", len(testing_list))\n",
        "print(\"Number of validation samples\", len(validation_list))\n",
        "\n",
        "## Construct a train list\n",
        "training_list = []\n",
        "for c in classes:\n",
        "  training_list += glob.glob(c + \"/*\")\n",
        "\n",
        "training_list = list(filter(lambda x : not x in testing_list and not x in validation_list, training_list))\n",
        "print(\"Number of training samples\", len(training_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXMxzxIZ0l_4"
      },
      "source": [
        "Now, we can create a custom SpeechDataset class that takes a file list in input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16BFbZcPmEUo"
      },
      "source": [
        "class SpeechDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, classes, file_list):\n",
        "\n",
        "    self.classes = classes\n",
        "\n",
        "    # create a map from class name to integer\n",
        "    self.class_to_int = dict(zip(classes, range(len(classes))))\n",
        "\n",
        "    # store the file names\n",
        "    self.samples = file_list\n",
        "\n",
        "    # store our MFCC transform\n",
        "    self.mfcc_transform = torchaudio.transforms.MFCC(n_mfcc=12, log_mels=True)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    with torch.no_grad():\n",
        "      # load a normalized waveform\n",
        "      waveform,_ = torchaudio.load(self.samples[i], normalize=True)\n",
        "\n",
        "      # if the waveform is too short (less than 1 second) we pad it with zeroes\n",
        "      if waveform.shape[1] < 16000:\n",
        "        waveform = F.pad(input=waveform, pad=(0, 16000 - waveform.shape[1]), mode='constant', value=0)\n",
        "\n",
        "      # then, we apply the transform\n",
        "      mfcc = self.mfcc_transform(waveform).squeeze(0).transpose(0,1)\n",
        "\n",
        "    # get the label from the file name\n",
        "    label = self.samples[i].split(\"/\")[0]\n",
        "\n",
        "    # return the mfcc coefficient with the sample label\n",
        "    return mfcc, self.class_to_int[label]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEh1Qpj51Kt2"
      },
      "source": [
        "## Q3: Create instances of the SpeechDataset for the train and val sets\n",
        "\n",
        "Fill the code below to create your Dataset objects for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6daZ3fGqAPH"
      },
      "source": [
        "train_set = ##YOUR CODE HERE\n",
        "val_set = ##YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_l-FHgwccx2"
      },
      "source": [
        "## Q4: Create Dataloaders for training and validation\n",
        "\n",
        "Fill the code below to create DataLoaders with the Datasets you just created.\n",
        "\n",
        "Do not forget to add shuffling to the training DataLoader.\n",
        "\n",
        "Print a batch of data to make sure everything works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cndnctmar0RM"
      },
      "source": [
        "train_dl = ##YOUR CODE HERE\n",
        "val_dl = ##YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Md2cucYmo0"
      },
      "source": [
        "# Part 2: Implementing a simple Recurrent Neural Network\n",
        "\n",
        "For our network, we are going to use an **RNN module** from torch.nn (which can have multiple layers, or cells).\n",
        "\n",
        "This module has an **input size**, which in our case will be equal to **the number of MFCC features (12)**. The input size is the number of dimensions of **x** in the image below.\n",
        "\n",
        "It also has an **hidden size**, which is the size of the output of the layers as well as the size of the internal representation of the features. We are going to choose **256** to start, but feel free to change that. This is the dimension of **h** in the image below.\n",
        "\n",
        "PyTorch RNN modules have a **number of layers**, which is simply the number of stacked **RNN Cells**. We are going to use 2 cells here, but feel free to change that as well. This is the **depth** in the image below.\n",
        "\n",
        "Then, in order to get as many output as the number of classes in our dataset, we need to have a **Linear layer** that goes from **256 inputs (the hidden size) to 30 outputs (the number of classes).**\n",
        "\n",
        "Finally, to output categorical probabilities, we use a **Softmax layer.**\n",
        "\n",
        "![alt text](https://i.stack.imgur.com/SjnTl.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd7MYqOfchC0"
      },
      "source": [
        "## Q4: Implement the network\n",
        "\n",
        "Fill the code below to implement the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4fzpQlXvlPG"
      },
      "source": [
        "class SpeechRNN(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SpeechRNN, self).__init__()\n",
        "\n",
        "    self.lstm = torch.nn.RNN(##YOUR CODE HERE\n",
        "\n",
        "    self.out_layer = ##YOUR CODE HERE\n",
        "\n",
        "    self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out, _ = self.lstm(x)\n",
        "\n",
        "    x = self.out_layer(out[:,-1,:])\n",
        "\n",
        "    return self.softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C49yQcCt21g2"
      },
      "source": [
        "Use this code to check that your implementation is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP7MB5iTzzey"
      },
      "source": [
        "net = SpeechRNN()\n",
        "batch = next(iter(train_dl))[0]\n",
        "print(batch.shape)\n",
        "y = net(batch)\n",
        "\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9YE3KoWYpSV"
      },
      "source": [
        "# Part 3: Training the network\n",
        "\n",
        "As usual, we need to define a loss and an optimizer. Since we have a categorical classification problem, we use cross-entropy (negative log likelihood).\n",
        "\n",
        "We can use the Adam optimizer, feel free to change it or the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGojns40_47n"
      },
      "source": [
        "##RE-RUN THIS CODE TO GET A \"NEW\" NETWORK\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "## Create an instance of our network\n",
        "net = SpeechRNN()\n",
        "\n",
        "## Move it to the GPU\n",
        "net = net.cuda()\n",
        "\n",
        "# Negative log likelihood loss\n",
        "criterion = torch.nn.NLLLoss()\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFRoVfl8corQ"
      },
      "source": [
        "## Q5: Training loop\n",
        "\n",
        "We also need to write a training loop. Fill the code below to create it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMy9ADUP_5ib"
      },
      "source": [
        "## NUMBER OF EPOCHS TO TRAIN\n",
        "N_EPOCHS = 5\n",
        "\n",
        "epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc = [], [], [], []\n",
        "\n",
        "for e in range(N_EPOCHS):\n",
        "\n",
        "  print(\"EPOCH:\",e)\n",
        "\n",
        "  ### TRAINING LOOP\n",
        "  running_loss = 0\n",
        "  running_accuracy = 0\n",
        "\n",
        "  ## Put the network in training mode\n",
        "  net.train()\n",
        "\n",
        "  for i, batch in enumerate(tqdm_notebook(train_dl)):\n",
        "\n",
        "  ##YOUR CODE HERE\n",
        "\n",
        "    ## Compute some statistics\n",
        "    with torch.no_grad():\n",
        "      running_loss += loss.item()\n",
        "      running_accuracy += (y.max(1)[1] == labels).sum().item()\n",
        "\n",
        "  print(\"Training accuracy:\", running_accuracy/float(len(train_set)),\n",
        "        \"Training loss:\", running_loss/float(len(train_set)))\n",
        "\n",
        "  epoch_loss.append(running_loss/len(train_set))\n",
        "  epoch_acc.append(running_accuracy/len(train_set))\n",
        "\n",
        "  ### VALIDATION LOOP\n",
        "  ## Put the network in validation mode\n",
        "  net.eval()\n",
        "\n",
        "  running_val_loss = 0\n",
        "  running_val_accuracy = 0\n",
        "\n",
        "  for i, batch in enumerate(val_dl):\n",
        "\n",
        "    with torch.no_grad():\n",
        "       #YOUR CODE HERE\n",
        "\n",
        "      running_val_loss += loss.item()\n",
        "      running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n",
        "\n",
        "  print(\"Validation accuracy:\", running_val_accuracy/float(len(val_set)),\n",
        "        \"Validation loss:\", running_val_loss/float(len(val_set)))\n",
        "\n",
        "  epoch_val_loss.append(running_val_loss/len(val_set))\n",
        "  epoch_val_acc.append(running_val_accuracy/len(val_set))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AB3fhIDZkcA"
      },
      "source": [
        "## Q6: From RNN to LSTM/GRU\n",
        "\n",
        "As you can see, the accuracy is pretty bad when we are only using \"regular\" RNNs. These are not used very much in practice nowadays because they do not have long-term memory. This means that by the time the network is done processing the whole audio sample, it probably has already forgotten the important parts of it. **Replace the RNN module in your network (Q4) with an LSTM or a GRU module (as you want). Train a new network and watch that accuracy go up!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we2NokjPSJyk"
      },
      "source": [
        "# Part 4: Evaluation\n",
        "\n",
        "Now, we need to evaluate our network on the test set.\n",
        "\n",
        "Use the code below to do that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MuCIQF0SOMp"
      },
      "source": [
        "# Create a test dataset instance\n",
        "test_dataset = SpeechDataset(classes, testing_list)\n",
        "\n",
        "# Create a DataLoader\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "net.eval()\n",
        "\n",
        "test_loss = 0\n",
        "test_accuracy = 0\n",
        "\n",
        "preds, y_test = np.array([]), np.array([])\n",
        "\n",
        "for i, batch in enumerate(test_dl):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Get a batch from the dataloader\n",
        "    x = batch[0]\n",
        "    labels = batch[1]\n",
        "\n",
        "    # move the batch to GPU\n",
        "    x = x.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    # Compute the network output\n",
        "    y = net(x)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(y, labels)\n",
        "\n",
        "    ## Store all the predictions an labels for later\n",
        "    preds = np.hstack([preds, y.max(1)[1].cpu().numpy()])\n",
        "    y_test = np.hstack([y_test, labels.cpu().numpy()])\n",
        "\n",
        "    test_loss += loss.item()\n",
        "    test_accuracy += (y.max(1)[1] == labels).sum().item()\n",
        "\n",
        "print(\"Test accuracy:\", test_accuracy/float(len(test_dataset)),\n",
        "      \"Test loss:\", test_loss/float(len(test_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cdeFL0Wb-Ro"
      },
      "source": [
        "## Confusion matrix\n",
        "\n",
        "In classification problems, it is common to use a confusion matrix to visualize which particular classes the model struggles with.\n",
        "\n",
        "Use the code below to generate a confusion matrix. What is the confusion that your network makes the most?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gjiSPncbzuS"
      },
      "source": [
        "def show_confusion_matrix(pred, Y_TEST, classes):\n",
        "\n",
        "  cm = confusion_matrix(y_true=Y_TEST, y_pred=pred)\n",
        "  cm = normalize(cm,axis=1,norm='l1')\n",
        "\n",
        "  df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
        "\n",
        "  plt.figure(figsize=(30,15))\n",
        "\n",
        "  cmap = sn.cubehelix_palette(light=1, as_cmap=True)\n",
        "  sn.heatmap(df_cm, annot=True,cmap=cmap)\n",
        "  plt.title('Confusion Matrix',fontdict={'fontsize':20})\n",
        "  plt.xlabel('Predicted labels')\n",
        "  plt.ylabel('True labels')\n",
        "  plt.show\n",
        "\n",
        "\n",
        "show_confusion_matrix(preds, y_test, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxpBRWsQcnJF"
      },
      "source": [
        "# (OPTIONAL) Part 5: Going further\n",
        "\n",
        "## Kaggle audio recognition datasets\n",
        "\n",
        "If you want to do more audio classification, there have been two competitions on Kaggle:\n",
        "\n",
        "- https://www.kaggle.com/c/freesound-audio-tagging\n",
        "\n",
        "- https://www.kaggle.com/c/freesound-audio-tagging-2019\n",
        "\n",
        "![alt text](https://storage.googleapis.com/kaggle-media/competitions/freesound/task2_freesound_audio_tagging.png)\n",
        "\n",
        "## More RNN things\n",
        "\n",
        "There are lots of things you can do with RNN. Natural Language Processing (NLP) is really popular right now.\n",
        "You can follow PyTorch tutorials to get started!\n",
        "\n",
        "https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html"
      ]
    }
  ]
}